### Part.1

We train the gpt-2 124M model, using parameters of gpt-2 and gpt-3 paper. The architecture is just the same as the paper.

The training and data processing code is learned in karpathy's video.

You could modify the batch size, node_num, EPOCH, and repo_id.